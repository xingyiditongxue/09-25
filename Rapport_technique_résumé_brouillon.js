第一部分
在第一部分中，我们将重点将当前主流的生成式 AI 模型分为三类：轻量级、中型和大型模型。

我们将介绍每种类型的性能特征（上下文窗口范围、响应速度、推理能力等），以及相关的硬件要求（RAM、VRAM、核心数量、功耗）。
除了研究聊天模型（尤其是普通用户可访问的模型）之外，我们还将分析嵌入模型，这些模型对于为企业、组织或个人构建 RAG 知识库至关重要。

我们的产品采用模型时的标准：
不用于训练 & 留存策略：对接外部厂商时，只用支持不默认用于训练与可配置留存/零留存的通道（例：OpenAI Enterprise/ChatGPT Enterprise、Azure OpenAI、Vertex AI 等）。

第二部分
本节将包含三个小节：2A. 私有云、2B. 本地物理服务器和 2C. 混合解决方案。

2A. 私有云
为客户搭建私有云以部署 LLM 的可行性。
根据私有云中可租用服务器的性能，我们需要详细说明：
哪些架构和成本支持轻量级模型、中级模型和大型模型的部署。

2B. 本地物理服务器
然后，我们将分析在本地数据中心内部署 LLM 的方案，并依赖专用物理服务器。
根据主板、GPU、CPU、电源、硬盘、内存条等的选择，我们将能够确定：
哪些硬件架构和购置成本支持轻量级模型、中级模型和大型 LLM。

2C. Solution Hybride

第三部分

3a. 与现有 AI 平台集成
借助数据抓取和 API，我们可以帮助用户访问市面上各种 AI 模型。
理论上，实施这项服务不需要任何特定的基础设施。它仅依赖于 AI 平台提供的 Web 服务和 API。
然而，值得思考的是，在现实/实际应用中，实施起来是否同样简单。

3b. 本地模型部署
我们还允许用户直接在自己的基础设施上部署和运行 LLM，从而完全掌控其日常使用。
这种方法消除了敏感数据或关键对话泄露给第三方服务的风险。

3c. RAG 和企业知识库支持
最后，我们协助客户创建本地知识库管理系统并实施 RAG（检索增强生成）解决方案。
这包括内部数据管理、调整/微调本地模型以更好地满足特定业务需求，以及整理具有战略意义或高价值的推理结果。

//数据边界：明确“控制平面 vs 数据平面”分离。控制平面（账号、计费、编排）由你管理；数据平面（文档、向量、推理）可选托管在你方、客户私有云 VPC/VNet，或完全本地机房。

